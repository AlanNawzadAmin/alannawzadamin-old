## Alan N. Amin
![Image](/assets/zUg6W86__400x400.jpg)

### About me

I am a 4th year PhD student in the Harvard Systems Biology program supervised by [Debora Marks](https://www.deboramarkslab.com/).
I graduated with a BS in Biochemistry and Mathematics from the University of Toronto in 2019.

I work on building statistical tools for biological sequence data: to optimize, compare, relate to (and infer) biophyiscal models, combine with experimental measurements (to interpret experimental results or design new ones), etc....
To do this with the scale and flexiblity necessary for modern sequence data, I usually work on nonparametric methods: a nonparametric Bayesian model and kernels.
But most of all, I work with [Eli](https://eweinstein.github.io/).

I've proved some results on convergence of stochastic processes in sequence space using Lyapunov function analysis (unfortunately relegated to the supplement of "A Kernelized Stein Discrepancy for Biological Sequences"). I've recently become interested in extending these results to build optimization and sampling methods with guarantees.

I'm also always asked how we can hope to learn extremely high dimensional sequence data with flexible models.
My methods so far rely on assumptions like good approximation with distributions in the support of a nonparamteric bayesian model or functions in an RKHS (which are a type of "low dimentionality" assumption).
What about the regimen where deep learning does well -- there are low dimensional "latent" factors that determine the data?
It'd be nice to look at adaptivity results for deep learning for sequence data (it's not obvious to me that this adaptivity is manifest when sequence aren't aligned for example).
So I've been recently interested in results on adaptivity.

**I'm also looking for a postdoc position!**

[A link to my CV.](https://github.com/AlanNawzadAmin/alannawzadamin.github.io/blob/main/assets/Alan_Amin_CV.pdf?raw=true)

### Selected works
* denotes equal contribution

**Amin A N**, Weinstein E N\*, Marks D S\* (*Equal contribution). A Kernelized Stein Discrepancy for Biological Sequences. Under Review, 2023.

**Amin A N**, Weinstein E N\*, Marks D S\* (*Equal contribution). Kernels with Guaranteed Flexibility for Reliable Machine Learning on Biological Sequences. arXiv, 2023. [paper](https://arxiv.org/abs/2304.03775)

Weinstein E N\*, **Amin A N\***, Frazer J, Marks D S (*Equal contribution). Non-identifiability and the blessings of misspecification in models of molecular fitness and phylogeny. NeurIPS, 2022 (Oral). [paper](https://proceedings.neurips.cc/paper_files/paper/2022/file/247e592848391fe01f153f179c595090-Paper-Conference.pdf)

**Amin A N\***, Weinstein E N\*, Marks D S (*Equal contribution). A generative nonparametric Bayesian model for whole genomes. NeurIPS, 2021. [paper](https://proceedings.neurips.cc/paper/2021/hash/e9dcb63ca828d0e00cd05b445099ed2e-Abstract.html)


### Contact
Reach me at alanamin@g.harvard.edu or 6173863043.


